{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmgFpZGoMOrxpBASVo/ktA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\"\"\"\n","This script evaluates the output of a finetuned Large Language Model (LLM)\n","using standard classification metrics: Accuracy, F1 Score, Precision, and Recall.\n","\n","It reads from a `.jsonl` file containing prediction-label pairs in the following format:\n","\n","{\"predict\": \"label1\", \"label\": \"label1\"}\n","\n","Metrics are computed using scikit-learn and support weighted averaging for multi-class outputs.\n","\n","Typical use case:\n","- After LLM inference/generation is saved to `generated_predictions.jsonl`\n","- This script will summarize performance metrics for analysis and reporting\n","\"\"\"\n"],"metadata":{"id":"qZzBhtoabEqK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpOpu5OQFd5l"},"outputs":[],"source":["import json\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","\n","predictions = []\n","labels = []\n","precision =[]\n","recall=[]\n","with open('generated_predictions.jsonl', 'r') as file:\n","    for line in file:\n","        entry = json.loads(line)\n","        predictions.append(entry['predict'])\n","        labels.append(entry['label'])\n","\n","accuracy = accuracy_score(labels, predictions)\n","print(f'Accuracy: {accuracy:.4f}')\n","\n","f1 = f1_score(labels, predictions, average='weighted')\n","print(f'F1 Score: {f1:.4f}')\n","\n","precision = precision_score(labels, predictions, average='weighted')\n","print(f'precision Score: {precision:.4f}')\n","\n","recall = recall_score(labels, predictions, average='weighted')\n","print(f'recall Score: {recall:.4f}')\n"]}]}